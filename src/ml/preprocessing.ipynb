{
   "cells": [
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Imports"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import numpy as np\n",
            "import pandas as pd\n",
            "import matplotlib.pyplot as plt\n",
            "from bs4 import BeautifulSoup\n",
            "import html\n",
            "import joblib\n",
            "import shutil\n",
            "from sklearn.feature_extraction.text import TfidfVectorizer\n",
            "from sklearn.decomposition import TruncatedSVD\n",
            "from sklearn.preprocessing import Normalizer, MultiLabelBinarizer\n",
            "from imblearn.over_sampling import SMOTE\n",
            "from imblearn.under_sampling import RandomUnderSampler, TomekLinks, NearMiss\n",
            "from imblearn.combine import SMOTETomek\n",
            "from imblearn.pipeline import make_pipeline\n",
            "from gensim.parsing.porter import PorterStemmer\n",
            "\n",
            "# modelling\n",
            "from sklearn.base import TransformerMixin\n",
            "from sklearn.model_selection import train_test_split\n",
            "from sklearn.ensemble import RandomForestClassifier\n",
            "\n",
            "# Evaluation\n",
            "from sklearn.model_selection import cross_val_score\n",
            "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
            "from sklearn.model_selection import RepeatedStratifiedKFold\n",
            "\n",
            "import nltk\n",
            "from nltk.corpus import stopwords\n",
            "from nltk.tokenize import word_tokenize\n",
            "from nltk.util import ngrams\n",
            "nltk.download('punkt')"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Helper class"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def evaluate_model(model, X, y, label):\n",
            "    \"\"\"\n",
            "    :param model: model to evaluate\n",
            "    :param X: features\n",
            "    :param y: target\n",
            "    :param label: label for the model \n",
            "\n",
            "    \"\"\"\n",
            "    y_pred = model.predict(X)\n",
            "\n",
            "    print(label + ' Set')\n",
            "    print(\"Accuracy:\", accuracy_score(y, y_pred))\n",
            "    print(\"F1 Score:\", f1_score(y, y_pred, average='macro'))\n",
            "    print()\n",
            "\n",
            "    print(\"Classification Report\")\n",
            "\n",
            "\n",
            "    print(classification_report(y, y_pred, digits=4))\n",
            "    \n",
            "\n",
            "\n",
            "def get_score(model, X, y):\n",
            "    \"\"\"\n",
            "    :param model: model to evaluate\n",
            "    :param X: features\n",
            "    :param y: target\n",
            "\n",
            "    \"\"\"\n",
            "    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
            "    print('Accuracy: ', cross_val_score(model, X, y, cv=cv, scoring='accuracy').mean())\n",
            "    print('Precision Macro: ', cross_val_score(model, X, y, cv=cv, scoring='precision_macro').mean())\n",
            "    print('Recall Macro: ', cross_val_score(model, X, y, cv=cv, scoring='recall_macro').mean())\n",
            "    print('F1 Macro: ', cross_val_score(model, X, y, cv=cv, scoring='f1_macro').mean())\n",
            "    \n",
            "def compress_file(input_file, output_tar_gz):\n",
            "    shutil.make_archive(output_tar_gz, 'xztar', '.', input_file)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "df = pd.read_csv('../datasets/emscad_v1.csv')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "df.columns"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Data Cleaning"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "df = df[['description', 'requirements', 'benefits', 'fraudulent']].fillna('')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "df[\"feature\"] = df['description'] + \" \"+ df['requirements'] + \" \" + df['benefits']"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "df = df[['feature', 'fraudulent']]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "df.head(5)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "df['feature'] = df['feature'].str.lower()\n",
            "df.head(5)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def remove_html_tags_and_escape_chars(input_text):\n",
            "    # Remove HTML tags\n",
            "    text_without_html = BeautifulSoup(input_text, 'html.parser').get_text()\n",
            "\n",
            "    # Unescape HTML characters\n",
            "    text_without_escape_chars = html.unescape(text_without_html)\n",
            "\n",
            "    return text_without_escape_chars"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "df['feature'] = df['feature'].apply(remove_html_tags_and_escape_chars)\n",
            "df.head(5)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def remove_non_alpha(input_text):\n",
            "    return ''.join(char if char.isalpha() or char.isspace() else ' ' for char in input_text)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "df['feature'] = df['feature'].apply(remove_non_alpha)\n",
            "df.head(5)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# tokenise\n",
            "df['feature'] = df['feature'].apply(lambda x: word_tokenize(x.lower()))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "df.head(5)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# remove stopwords\n",
            "all_stopwords = set(stopwords.words('english'))\n",
            "all_stopwords.update(['\\\\r\\\\n'])\n",
            "df['feature'] = df['feature'].apply(lambda x: [word for word in x if word not in all_stopwords])"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "df.head(5)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# stem words\n",
            "df['feature'] = df['feature'].apply(lambda x: [PorterStemmer().stem(word) for word in x])"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "df.head(5)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "df['feature'] = df['feature'].apply(lambda x: [word for word in x if len(word) >= 3])"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "df.head(5)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "df['feature'] = df['feature'].apply(lambda x: ' '.join(x))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# drop rows wwith empty str\n",
            "df = df[df['feature'] != '']"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "df['fraudulent'] = df['fraudulent'].apply(lambda x: 1 if x == \"t\" else 0)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "df.head(5)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Feature extraction using tf-idf"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Fit and transform the text data using TF-IDF\n",
            "tfidf = TfidfVectorizer()\n",
            "dtm = tfidf.fit_transform(df['feature'])"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Dimensionsality reduction using SVD <br>\n",
            "This removes the less important variables in my dataset and improves training speed."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "dimension = 350\n",
            "svd = TruncatedSVD(dimension, random_state=42)\n",
            "dtm_svd = svd.fit_transform(dtm)\n",
            "# Apply Normalizer to normalize the data\n",
            "dtm_svd_normalized = Normalizer(copy=False)\n",
            "dtm_svd_normalized = dtm_svd_normalized.fit_transform(dtm_svd)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "x = pd.DataFrame(dtm_svd)\n",
            "x.reset_index(inplace=True, drop=True)\n",
            "y = df['fraudulent']"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# train test split\n",
            "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42, stratify=y)"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Modelling"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
            "rf.fit(x_train, y_train)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "evaluate_model(rf, x_train, y_train, 'Train')\n",
            "evaluate_model(rf, x_test, y_test, 'Test')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# use SMOTETomek to oversample the minority class\n",
            "x_res, y_res = SMOTETomek(sampling_strategy='all', random_state=42).fit_resample(x, y)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# train test split\n",
            "x_train, x_test, y_train, y_test = train_test_split(x_res, y_res, test_size=0.2, random_state=42, stratify=y_res)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
            "rf.fit(x_train, y_train)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "evaluate_model(rf, x_train, y_train, 'Train')\n",
            "evaluate_model(rf, x_test, y_test, 'Test')"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Create a pipeline for the model"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "test_df = df"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# make pipeline\n",
            "tfidf = TfidfVectorizer()\n",
            "svd = TruncatedSVD(n_components=350, random_state=42)\n",
            "smote = SMOTETomek(sampling_strategy='all', random_state=42)\n",
            "norm = Normalizer(copy=False)\n",
            "rf = RandomForestClassifier(n_estimators=300, random_state=42)\n",
            "pipe = make_pipeline(tfidf, svd, smote, norm, rf)\n",
            "x_train, x_test, y_train, y_test = train_test_split(\n",
            "    df['feature'], df['fraudulent'], test_size=0.2, random_state=42, stratify=df['fraudulent'])"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "pipe.fit(x_train, y_train)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "evaluate_model(pipe, x_train, y_train, 'Train')\n",
            "evaluate_model(pipe, x_test, y_test, 'Test')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "joblib.dump(pipe, '../models/rf.pkl')"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### CNN Attempt"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import tensorflow as tf\n",
            "from keras import layers, models\n",
            "from keras.callbacks import EarlyStopping, LearningRateScheduler\n",
            "from keras.regularizers import l2\n",
            "from keras.models import Sequential\n",
            "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Activation, BatchNormalization\n",
            "from keras.preprocessing.image import ImageDataGenerator"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# make pipeline\n",
            "tfidf = TfidfVectorizer()\n",
            "svd = TruncatedSVD(n_components=350, random_state=42)\n",
            "smote = SMOTETomek(sampling_strategy='all', random_state=42)\n",
            "norm = Normalizer(copy=False)\n",
            "pipe = make_pipeline(tfidf, svd, smote, norm)\n",
            "x_train, x_test, y_train, y_test = train_test_split(\n",
            "    df['feature'], df['fraudulent'], test_size=0.2, random_state=42, stratify=df['fraudulent'])\n",
            "pipe.fit(x_train, y_train)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "new_x_train = pipe.transform(x_train)\n",
            "new_x_test = pipe.transform(x_test)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "basic_adam_model = models.Sequential()\n",
            "basic_adam_model.add(layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
            "basic_adam_model.add(layers.MaxPooling2D((2, 2)))\n",
            "basic_adam_model.add(layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
            "basic_adam_model.add(layers.MaxPooling2D((2, 2)))\n",
            "basic_adam_model.add(layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
            "basic_adam_model.add(layers.MaxPooling2D((2, 2)))\n",
            "basic_adam_model.add(layers.Flatten())\n",
            "basic_adam_model.add(layers.Dense(64, activation='relu'))\n",
            "basic_adam_model.add(layers.Dense(10, activation='sigmoid'))\n",
            "\n",
            "basic_adam_model.compile(optimizer='adam',\n",
            "loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
            "metrics=['accuracy'])\n",
            "basic_adam_model.build(input_shape=(None, 32, 32, 1))\n",
            "basic_adam_model.summary()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "batch_size = 64\n",
            "epochs = 100\n",
            "\n",
            "es_callback = EarlyStopping(monitor='val_loss', mode='min', patience=5)\n",
            "\n",
            "basic_adam_model_history = basic_adam_model.fit(new_x_train.reshape(-1, 32, 32, 1), y_train,\n",
            "                                                validation_data=(new_x_test.reshape(-1, 32, 32, 1), y_test),\n",
            "                                                batch_size=batch_size, epochs=epochs, callbacks=[es_callback])"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "base",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.10.0"
      },
      "orig_nbformat": 4,
      "vscode": {
         "interpreter": {
            "hash": "f48b1a2c38788c4a91ce3e7692fc5e2582621afc45073ba526ca796c7ef760f3"
         }
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
